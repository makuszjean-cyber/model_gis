{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "222d77fc",
   "metadata": {},
   "source": [
    "# D√©tection de palmiers + maisons avec YOLOv8 ‚Äî v2 + Export ONNX pour Deepness (QGIS)\n",
    "\n",
    "**Version 2** ‚Äî Entra√Ænement multi-classes (palmier + maison).\n",
    "\n",
    "> La v1 ne d√©tectait qu'une seule classe (palmier). Cette v2 entra√Æne sur **2 classes** :\n",
    "> - Classe 0 : **palmier**\n",
    "> - Classe 1 : **maison**\n",
    "\n",
    "Ce notebook permet de :\n",
    "1. Monter Google Drive et acc√©der au dataset\n",
    "2. Installer les d√©pendances n√©cessaires\n",
    "3. **Diagnostiquer** le dataset (labels, tailles de bboxes, distribution des classes)\n",
    "4. Nettoyer les labels (BOM UTF-8) **sans remapper les classes** (on conserve 0 et 1)\n",
    "5. Convertir les images GeoTIFF (RGBA) ‚Üí PNG (RGB)\n",
    "6. Entra√Æner un mod√®le **YOLOv8n** sur 2 classes\n",
    "7. Exporter le mod√®le au format ONNX\n",
    "8. Ajouter les m√©tadonn√©es Deepness (plugin QGIS) au fichier ONNX\n",
    "9. Sauvegarder le mod√®le final dans Google Drive\n",
    "\n",
    "**Dataset** : ~110 images (80/20 train/val), ~4700 palmiers, ~2100 maisons, ~6857 annotations au total.\n",
    "\n",
    "**Pr√©requis** : le dataset doit √™tre dans Google Drive avec la structure suivante :\n",
    "```\n",
    "dataset_palmiers/\n",
    "‚îú‚îÄ‚îÄ images/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ train/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ val/\n",
    "‚îú‚îÄ‚îÄ labels/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ train/   (classe 0 = palmier, classe 1 = maison)\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ val/\n",
    "‚îî‚îÄ‚îÄ palms.yaml   (nc: 2, names: ['palmier', 'maison'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fea908",
   "metadata": {},
   "source": [
    "## 1. Monter Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5df280",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac18b97",
   "metadata": {},
   "source": [
    "## 2. Installer les d√©pendances\n",
    "\n",
    "On installe `ultralytics` (YOLOv8) et `onnx` (pour manipuler les m√©tadonn√©es du mod√®le export√©)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e051e3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U ultralytics onnx --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345695cb",
   "metadata": {},
   "source": [
    "## 3. D√©finir le chemin du dataset et mettre √† jour `palms.yaml`\n",
    "\n",
    "On met √† jour le fichier `palms.yaml` pour que le chemin (`path`) pointe vers le dossier dans Google Drive.\n",
    "\n",
    "Le fichier `palms.yaml` doit contenir :\n",
    "- `nc: 2` (2 classes)\n",
    "- `names: ['palmier', 'maison']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f07722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "# --- Trouver automatiquement le dataset dans Google Drive ---\n",
    "DRIVE_ROOT = '/content/drive/MyDrive'\n",
    "\n",
    "# Chemins possibles (ajoutez le v√¥tre si diff√©rent)\n",
    "candidates = [\n",
    "    os.path.join(DRIVE_ROOT, 'Recherche', 'dataset_palmiers'),\n",
    "    os.path.join(DRIVE_ROOT, 'dataset_palmiers'),\n",
    "    os.path.join(DRIVE_ROOT, 'recherche', 'dataset_palmiers'),\n",
    "]\n",
    "\n",
    "DATASET_DIR = None\n",
    "for path in candidates:\n",
    "    if os.path.isdir(path):\n",
    "        DATASET_DIR = path\n",
    "        break\n",
    "\n",
    "# Si aucun candidat trouv√©, afficher le contenu du Drive pour aider\n",
    "if DATASET_DIR is None:\n",
    "    print(\"‚ùå Dataset non trouv√© dans les chemins suivants :\")\n",
    "    for p in candidates:\n",
    "        print(f\"   {p}\")\n",
    "    print(f\"\\nüìÇ Contenu de {DRIVE_ROOT} :\")\n",
    "    for item in sorted(os.listdir(DRIVE_ROOT)):\n",
    "        full = os.path.join(DRIVE_ROOT, item)\n",
    "        marker = 'üìÅ' if os.path.isdir(full) else 'üìÑ'\n",
    "        print(f\"   {marker} {item}\")\n",
    "    # Chercher r√©cursivement un dossier dataset_palmiers\n",
    "    print(\"\\nüîç Recherche de 'dataset_palmiers' dans le Drive...\")\n",
    "    for root, dirs, files in os.walk(DRIVE_ROOT):\n",
    "        if 'dataset_palmiers' in dirs:\n",
    "            found = os.path.join(root, 'dataset_palmiers')\n",
    "            print(f\"   ‚úÖ Trouv√© : {found}\")\n",
    "    raise FileNotFoundError(\"Modifiez DATASET_DIR avec le bon chemin ci-dessus.\")\n",
    "\n",
    "YAML_PATH = os.path.join(DATASET_DIR, 'palms.yaml')\n",
    "assert os.path.isfile(YAML_PATH), f\"Le fichier palms.yaml n'existe pas dans {DATASET_DIR}\"\n",
    "\n",
    "print(f\"‚úÖ Dataset trouv√© : {DATASET_DIR}\")\n",
    "\n",
    "# Mettre √† jour le path dans palms.yaml pour pointer vers Colab\n",
    "with open(YAML_PATH, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "config['path'] = DATASET_DIR\n",
    "\n",
    "with open(YAML_PATH, 'w') as f:\n",
    "    yaml.dump(config, f, default_flow_style=False, allow_unicode=True)\n",
    "\n",
    "print(\"Configuration du dataset :\")\n",
    "print(yaml.dump(config, default_flow_style=False, allow_unicode=True))\n",
    "\n",
    "# V√©rifier que nc=2 et que les 2 classes sont d√©finies\n",
    "assert config.get('nc') == 2, f\"‚ùå nc devrait √™tre 2, trouv√© : {config.get('nc')}\"\n",
    "assert config.get('names') == ['palmier', 'maison'], f\"‚ùå names incorrect : {config.get('names')}\"\n",
    "print(\"‚úÖ palms.yaml correctement configur√© pour 2 classes : palmier + maison\")\n",
    "\n",
    "# V√©rifier la pr√©sence des images et labels\n",
    "for split in ['train', 'val']:\n",
    "    img_dir = os.path.join(DATASET_DIR, 'images', split)\n",
    "    lbl_dir = os.path.join(DATASET_DIR, 'labels', split)\n",
    "    n_img = len([f for f in os.listdir(img_dir) if f.endswith(('.tif', '.tiff', '.png', '.jpg'))])\n",
    "    n_lbl = len([f for f in os.listdir(lbl_dir) if f.endswith('.txt')])\n",
    "    print(f\"  {split}: {n_img} images, {n_lbl} labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823e2b82",
   "metadata": {},
   "source": [
    "## 4. Diagnostic du dataset (v2 ‚Äî multi-classes)\n",
    "\n",
    "Avant d'entra√Æner, on v√©rifie la qualit√© du dataset :\n",
    "- Labels vides ou manquants\n",
    "- **Distribution des classes** (palmier vs maison)\n",
    "- Distribution des tailles de bounding boxes par classe\n",
    "- Nombre d'objets par image\n",
    "\n",
    "Cela permet de d√©tecter des probl√®mes en amont (annotations incorrectes, d√©s√©quilibre de classes, objets trop petits, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206020c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "CLASS_NAMES = {0: 'palmier', 1: 'maison'}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DIAGNOSTIC DU DATASET ‚Äî v2 (palmier + maison)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Par classe : widths, heights\n",
    "class_widths = {0: [], 1: []}\n",
    "class_heights = {0: [], 1: []}\n",
    "class_counts = Counter()\n",
    "objects_per_image = []\n",
    "empty_labels, missing_labels = [], []\n",
    "\n",
    "for split in ['train', 'val']:\n",
    "    img_dir = os.path.join(DATASET_DIR, 'images', split)\n",
    "    lbl_dir = os.path.join(DATASET_DIR, 'labels', split)\n",
    "\n",
    "    images = sorted(glob.glob(os.path.join(img_dir, '*.*')))\n",
    "    print(f\"\\n--- {split.upper()} ---\")\n",
    "    print(f\"  Images trouv√©es : {len(images)}\")\n",
    "\n",
    "    for img_path in images:\n",
    "        base = os.path.splitext(os.path.basename(img_path))[0]\n",
    "        lbl_path = os.path.join(lbl_dir, base + '.txt')\n",
    "\n",
    "        if not os.path.exists(lbl_path):\n",
    "            missing_labels.append(f\"{split}/{base}\")\n",
    "            objects_per_image.append(0)\n",
    "            continue\n",
    "\n",
    "        with open(lbl_path, 'r') as f:\n",
    "            lines = [l.strip() for l in f.readlines() if l.strip()]\n",
    "\n",
    "        if len(lines) == 0:\n",
    "            empty_labels.append(f\"{split}/{base}\")\n",
    "            objects_per_image.append(0)\n",
    "            continue\n",
    "\n",
    "        objects_per_image.append(len(lines))\n",
    "        for line in lines:\n",
    "            parts = line.split()\n",
    "            if len(parts) >= 5:\n",
    "                cls = int(parts[0])\n",
    "                w, h = float(parts[3]), float(parts[4])\n",
    "                class_counts[cls] += 1\n",
    "                if cls in class_widths:\n",
    "                    class_widths[cls].append(w)\n",
    "                    class_heights[cls].append(h)\n",
    "\n",
    "if missing_labels:\n",
    "    print(f\"\\n‚ö†Ô∏è  Labels MANQUANTS ({len(missing_labels)}) : {missing_labels[:10]}\")\n",
    "if empty_labels:\n",
    "    print(f\"‚ö†Ô∏è  Labels VIDES ({len(empty_labels)}) : {empty_labels[:10]}\")\n",
    "if not missing_labels and not empty_labels:\n",
    "    print(\"\\n‚úÖ Tous les labels sont pr√©sents et non-vides.\")\n",
    "\n",
    "total_objects = sum(class_counts.values())\n",
    "print(f\"\\nüìä Statistiques :\")\n",
    "print(f\"  Total d'objets annot√©s : {total_objects}\")\n",
    "for cls_id in sorted(class_counts.keys()):\n",
    "    name = CLASS_NAMES.get(cls_id, f'classe_{cls_id}')\n",
    "    count = class_counts[cls_id]\n",
    "    pct = count / total_objects * 100\n",
    "    print(f\"  Classe {cls_id} ({name}) : {count} ({pct:.1f}%)\")\n",
    "print(f\"  Objets/image ‚Äî min: {min(objects_per_image)}, max: {max(objects_per_image)}, \"\n",
    "      f\"moyenne: {np.mean(objects_per_image):.1f}, m√©diane: {np.median(objects_per_image):.0f}\")\n",
    "\n",
    "# V√©rifier les classes inattendues\n",
    "unexpected = [c for c in class_counts if c not in CLASS_NAMES]\n",
    "if unexpected:\n",
    "    print(f\"\\n‚ùå Classes inattendues d√©tect√©es : {unexpected}\")\n",
    "    print(\"   ‚Üí V√©rifiez vos labels ! Seules les classes 0 (palmier) et 1 (maison) sont attendues.\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Seules les classes attendues (0, 1) sont pr√©sentes.\")\n",
    "\n",
    "# --- Graphiques ---\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Distribution des classes\n",
    "classes = sorted(class_counts.keys())\n",
    "counts = [class_counts[c] for c in classes]\n",
    "labels = [CLASS_NAMES.get(c, str(c)) for c in classes]\n",
    "colors = ['steelblue', 'coral']\n",
    "axes[0, 0].bar(labels, counts, color=colors[:len(classes)], edgecolor='white')\n",
    "axes[0, 0].set_title(\"Distribution des classes\")\n",
    "axes[0, 0].set_ylabel(\"Nombre d'annotations\")\n",
    "for i, (lbl, cnt) in enumerate(zip(labels, counts)):\n",
    "    axes[0, 0].text(i, cnt + 20, str(cnt), ha='center', fontweight='bold')\n",
    "\n",
    "# Objets par image\n",
    "axes[0, 1].hist(objects_per_image, bins=30, color='steelblue', edgecolor='white')\n",
    "axes[0, 1].set_title(\"Objets par image\")\n",
    "axes[0, 1].set_xlabel(\"Nombre d'objets\")\n",
    "axes[0, 1].set_ylabel(\"Nombre d'images\")\n",
    "\n",
    "# Tailles de bbox par classe ‚Äî largeur\n",
    "for cls_id in sorted(class_widths.keys()):\n",
    "    if class_widths[cls_id]:\n",
    "        axes[1, 0].hist(class_widths[cls_id], bins=50, alpha=0.6,\n",
    "                        label=f\"{CLASS_NAMES.get(cls_id, str(cls_id))} (n={len(class_widths[cls_id])})\")\n",
    "axes[1, 0].set_title(\"Largeur des bboxes (normalis√©e)\")\n",
    "axes[1, 0].set_xlabel(\"Largeur\")\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Tailles de bbox par classe ‚Äî hauteur\n",
    "for cls_id in sorted(class_heights.keys()):\n",
    "    if class_heights[cls_id]:\n",
    "        axes[1, 1].hist(class_heights[cls_id], bins=50, alpha=0.6,\n",
    "                        label=f\"{CLASS_NAMES.get(cls_id, str(cls_id))} (n={len(class_heights[cls_id])})\")\n",
    "axes[1, 1].set_title(\"Hauteur des bboxes (normalis√©e)\")\n",
    "axes[1, 1].set_xlabel(\"Hauteur\")\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Alerte si bboxes tr√®s petites\n",
    "all_widths = class_widths[0] + class_widths[1]\n",
    "all_heights = class_heights[0] + class_heights[1]\n",
    "small_threshold = 0.02\n",
    "n_small = sum(1 for w, h in zip(all_widths, all_heights) if w < small_threshold or h < small_threshold)\n",
    "if n_small > 0:\n",
    "    pct = n_small / len(all_widths) * 100\n",
    "    print(f\"\\n‚ö†Ô∏è  {n_small} bboxes ({pct:.1f}%) sont tr√®s petites (<{small_threshold*100}% de l'image).\")\n",
    "    print(f\"   ‚Üí La taille d'entr√©e imgsz=640 aidera √† les d√©tecter.\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Toutes les bboxes ont une taille raisonnable.\")\n",
    "\n",
    "# Ratio de d√©s√©quilibre\n",
    "if len(class_counts) == 2:\n",
    "    ratio = max(counts) / min(counts)\n",
    "    if ratio > 3:\n",
    "        print(f\"‚ö†Ô∏è  D√©s√©quilibre de classes (ratio {ratio:.1f}:1). Consid√©rez des poids de classe ou de l'oversampling.\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Ratio de classes acceptable ({ratio:.1f}:1).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c1be95",
   "metadata": {},
   "source": [
    "## 5. Nettoyer les labels (BOM UTF-8 uniquement)\n",
    "\n",
    "**Diff√©rence avec la v1** : en v1, toutes les classes √©taient remapp√©es vers 0 (palmier unique).\n",
    "En v2, on **conserve les classes 0 et 1** telles quelles. On ne corrige que le BOM UTF-8 √©ventuel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920f2638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "bom_fixed = 0\n",
    "invalid_classes = 0\n",
    "\n",
    "VALID_CLASSES = {0, 1}  # palmier=0, maison=1\n",
    "\n",
    "for split in ['train', 'val']:\n",
    "    lbl_dir = os.path.join(DATASET_DIR, 'labels', split)\n",
    "    for lbl_file in sorted(glob.glob(os.path.join(lbl_dir, '*.txt'))):\n",
    "        # Lire en binaire pour d√©tecter le BOM\n",
    "        with open(lbl_file, 'rb') as f:\n",
    "            raw = f.read()\n",
    "\n",
    "        # Supprimer le BOM UTF-8 (EF BB BF)\n",
    "        had_bom = raw.startswith(b'\\xef\\xbb\\xbf')\n",
    "        if had_bom:\n",
    "            raw = raw[3:]\n",
    "            bom_fixed += 1\n",
    "\n",
    "        # D√©coder et v√©rifier les classes (sans remapper)\n",
    "        text = raw.decode('utf-8')\n",
    "        new_lines = []\n",
    "        for line in text.strip().split('\\n'):\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 5:\n",
    "                cls = int(parts[0])\n",
    "                if cls not in VALID_CLASSES:\n",
    "                    print(f\"  ‚ö†Ô∏è  Classe invalide {cls} dans {os.path.basename(lbl_file)}\")\n",
    "                    invalid_classes += 1\n",
    "            new_lines.append(' '.join(parts))\n",
    "\n",
    "        # R√©√©crire seulement si BOM d√©tect√©\n",
    "        if had_bom:\n",
    "            with open(lbl_file, 'w', encoding='utf-8', newline='\\n') as f:\n",
    "                f.write('\\n'.join(new_lines) + '\\n')\n",
    "\n",
    "# Supprimer les fichiers .cache corrompus\n",
    "for cache_file in glob.glob(os.path.join(DATASET_DIR, 'labels', '*.cache')):\n",
    "    os.remove(cache_file)\n",
    "    print(f\"  Cache supprim√© : {cache_file}\")\n",
    "\n",
    "print(f\"\\n‚úÖ BOM supprim√© de {bom_fixed} fichiers.\")\n",
    "if invalid_classes > 0:\n",
    "    print(f\"‚ùå {invalid_classes} annotations avec des classes invalides ! V√©rifiez vos labels.\")\n",
    "else:\n",
    "    print(\"‚úÖ Toutes les annotations utilisent des classes valides (0=palmier, 1=maison).\")\n",
    "print(\"   Les labels sont maintenant pr√™ts pour YOLO v2.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d6b38e",
   "metadata": {},
   "source": [
    "## 6. Convertir les images RGBA ‚Üí RGB\n",
    "\n",
    "Les GeoTIFF ont 4 canaux (RGBA) mais YOLOv8 attend 3 canaux (RGB). On convertit toutes les images en supprimant le canal alpha, et on les sauvegarde en PNG (format standard pour YOLO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6dc1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "converted = 0\n",
    "for split in ['train', 'val']:\n",
    "    img_dir = os.path.join(DATASET_DIR, 'images', split)\n",
    "    for img_path in sorted(glob.glob(os.path.join(img_dir, '*.tif'))):\n",
    "        img = Image.open(img_path)\n",
    "        # Convertir en RGB si l'image a un canal alpha (RGBA)\n",
    "        if img.mode == 'RGBA':\n",
    "            img = img.convert('RGB')\n",
    "        elif img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "\n",
    "        # Sauvegarder en PNG (m√™me nom, extension .png)\n",
    "        png_path = os.path.splitext(img_path)[0] + '.png'\n",
    "        img.save(png_path)\n",
    "        converted += 1\n",
    "\n",
    "    # Supprimer les anciens .tif pour √©viter les doublons\n",
    "    for tif_path in glob.glob(os.path.join(img_dir, '*.tif')):\n",
    "        os.remove(tif_path)\n",
    "\n",
    "# Supprimer les .cache pour forcer YOLO √† rescanner les nouvelles images\n",
    "for cache_file in glob.glob(os.path.join(DATASET_DIR, 'labels', '*.cache')):\n",
    "    os.remove(cache_file)\n",
    "\n",
    "print(f\"‚úÖ {converted} images converties de RGBA/TIF ‚Üí RGB/PNG.\")\n",
    "print(f\"   Les fichiers .tif originaux ont √©t√© supprim√©s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c961d2a7",
   "metadata": {},
   "source": [
    "## 7. Entra√Æner le mod√®le YOLOv8n ‚Äî v2 (palmier + maison)\n",
    "\n",
    "Avec ~110 images et 2 classes, on utilise **YOLOv8n** (nano, 3M params) avec une strat√©gie de **fine-tuning** :\n",
    "\n",
    "**Adaptations :**\n",
    "- `yolov8n.pt` : mod√®le l√©ger ‚Üí moins de risque d'overfitting\n",
    "- `imgsz=640` : proche du natif ~680px\n",
    "- `freeze=10` : geler le backbone pr√©-entra√Æn√©, ne fine-tuner que la t√™te de d√©tection\n",
    "- `lr0=0.001` : learning rate bas pour du fine-tuning\n",
    "- `copy_paste=0.3` : copie-colle d'objets entre images (augmentation tr√®s efficace)\n",
    "- `mixup=0.15` : m√©lange d'images pour r√©gulariser\n",
    "- `epochs=200` / `patience=30` : plus de temps avec LR bas\n",
    "\n",
    "**Diff√©rence avec la v1 :** le mod√®le apprend maintenant √† distinguer 2 classes (palmier et maison)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f788fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# YOLO26n (nano) ‚Äî adapt√© au dataset (~110 images, 2 classes)\n",
    "model = YOLO('yolo26n.pt')\n",
    "\n",
    "# Entra√Ænement optimis√© pour petit dataset + vue a√©rienne + 2 classes\n",
    "results = model.train(\n",
    "    data=YAML_PATH,\n",
    "    epochs=200,\n",
    "    imgsz=640,           # proche du natif ~680px\n",
    "    batch=8,\n",
    "    project=os.path.join(DATASET_DIR, 'runs'),\n",
    "    name='palmier_maison_detect',\n",
    "    exist_ok=True,\n",
    "    # --- Fine-tuning ---\n",
    "    freeze=10,           # geler le backbone (couches 0-9), fine-tuner la t√™te\n",
    "    lr0=0.001,           # LR bas pour fine-tuning\n",
    "    lrf=0.01,            # LR final = lr0 * lrf\n",
    "    cos_lr=True,         # cosine annealing\n",
    "    # --- Early stopping ---\n",
    "    patience=30,         # 30 √©poques sans am√©lioration ‚Üí arr√™t\n",
    "    # --- Augmentations vue a√©rienne ---\n",
    "    degrees=90.0,        # rotation ¬±90¬∞ (vue z√©nithale)\n",
    "    flipud=0.5,          # retournement vertical\n",
    "    fliplr=0.5,          # retournement horizontal\n",
    "    scale=0.3,           # multi-scale mod√©r√©\n",
    "    mosaic=1.0,          # mosaic activ√©\n",
    "    close_mosaic=20,     # d√©sactiver mosaic les 20 derni√®res √©poques\n",
    "    copy_paste=0.3,      # copie-colle d'objets entre images\n",
    "    mixup=0.15,          # m√©lange d'images pour r√©gulariser\n",
    "    # --- Sauvegarde ---\n",
    "    save=True,\n",
    "    plots=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4429f951",
   "metadata": {},
   "source": [
    "## 8. Visualiser les r√©sultats d'entra√Ænement\n",
    "\n",
    "Afficher les courbes de loss et les m√©triques de validation pour les 2 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0721b52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "run_dir = os.path.join(DATASET_DIR, 'runs', 'palmier_maison_detect')\n",
    "\n",
    "# Afficher les courbes de r√©sultats\n",
    "results_img = os.path.join(run_dir, 'results.png')\n",
    "if os.path.exists(results_img):\n",
    "    display(Image(filename=results_img, width=900))\n",
    "else:\n",
    "    print(\"Fichier results.png non trouv√©.\")\n",
    "\n",
    "# Afficher la matrice de confusion (2 classes + background)\n",
    "conf_img = os.path.join(run_dir, 'confusion_matrix.png')\n",
    "if os.path.exists(conf_img):\n",
    "    display(Image(filename=conf_img, width=600))\n",
    "\n",
    "# Afficher des pr√©dictions sur le set de validation\n",
    "val_img = os.path.join(run_dir, 'val_batch0_pred.png')\n",
    "if os.path.exists(val_img):\n",
    "    display(Image(filename=val_img, width=900))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ff7a51",
   "metadata": {},
   "source": [
    "## 9. √âvaluation d√©taill√©e sur le set de validation\n",
    "\n",
    "On lance une √©valuation formelle avec le meilleur mod√®le pour obtenir les m√©triques pr√©cises **par classe** (mAP50, mAP50-95, pr√©cision, rappel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3945b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le meilleur mod√®le et lancer la validation\n",
    "run_dir = os.path.join(DATASET_DIR, 'runs', 'palmier_maison_detect')\n",
    "best_model_path = os.path.join(run_dir, 'weights', 'best.pt')\n",
    "\n",
    "val_model = YOLO(best_model_path)\n",
    "metrics = val_model.val(data=YAML_PATH, imgsz=640, split='val')\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"M√âTRIQUES DE VALIDATION ‚Äî v2 (palmier + maison)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n--- R√©sultats globaux ---\")\n",
    "print(f\"  Pr√©cision (P)   : {metrics.box.mp:.4f}\")\n",
    "print(f\"  Rappel (R)      : {metrics.box.mr:.4f}\")\n",
    "print(f\"  mAP@50          : {metrics.box.map50:.4f}\")\n",
    "print(f\"  mAP@50-95       : {metrics.box.map:.4f}\")\n",
    "\n",
    "# M√©triques par classe\n",
    "print(f\"\\n--- R√©sultats par classe ---\")\n",
    "class_names = ['palmier', 'maison']\n",
    "for i, name in enumerate(class_names):\n",
    "    print(f\"  {name}:\")\n",
    "    print(f\"    AP@50     : {metrics.box.ap50[i]:.4f}\")\n",
    "    print(f\"    AP@50-95  : {metrics.box.ap[i]:.4f}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Interpr√©tation\n",
    "map50 = metrics.box.map50\n",
    "if map50 >= 0.85:\n",
    "    print(\"‚úÖ Excellent ! Le mod√®le d√©tecte tr√®s bien les palmiers et maisons.\")\n",
    "elif map50 >= 0.70:\n",
    "    print(\"üëç Bon r√©sultat. Peut √™tre am√©lior√© avec plus de donn√©es ou plus d'√©poques.\")\n",
    "elif map50 >= 0.50:\n",
    "    print(\"‚ö†Ô∏è  R√©sultat moyen. V√©rifiez la qualit√© des annotations et augmentez les donn√©es.\")\n",
    "else:\n",
    "    print(\"‚ùå R√©sultat faible. Le dataset ou les annotations n√©cessitent une r√©vision.\")\n",
    "\n",
    "# V√©rifier si une classe est significativement plus faible\n",
    "ap50_diff = abs(metrics.box.ap50[0] - metrics.box.ap50[1])\n",
    "if ap50_diff > 0.15:\n",
    "    weaker = class_names[0] if metrics.box.ap50[0] < metrics.box.ap50[1] else class_names[1]\n",
    "    print(f\"‚ö†Ô∏è  La classe '{weaker}' est significativement plus faible. Ajoutez des annotations pour cette classe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b54bd9",
   "metadata": {},
   "source": [
    "## 10. Exporter le meilleur mod√®le en ONNX\n",
    "\n",
    "On charge le meilleur poids (`best.pt`) et on l'exporte au format ONNX avec `imgsz=640` (m√™me taille que l'entra√Ænement)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555a01e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le meilleur mod√®le entra√Æn√©\n",
    "best_model_path = os.path.join(run_dir, 'weights', 'best.pt')\n",
    "assert os.path.isfile(best_model_path), f\"Mod√®le introuvable : {best_model_path}\"\n",
    "\n",
    "best_model = YOLO(best_model_path)\n",
    "\n",
    "# Exporter en ONNX (opset=17 pour compatibilit√© avec Deepness/QGIS)\n",
    "onnx_path = best_model.export(format='onnx', imgsz=640, opset=17)\n",
    "print(f\"Mod√®le ONNX export√© : {onnx_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1a41ea",
   "metadata": {},
   "source": [
    "## 11. Ajouter les m√©tadonn√©es Deepness au fichier ONNX\n",
    "\n",
    "Le plugin **Deepness** pour QGIS attend des m√©tadonn√©es sp√©cifiques dans le mod√®le ONNX.\n",
    "On les ajoute ici directement dans le fichier.\n",
    "\n",
    "**v2** : `class_names` contient maintenant 2 classes : `{0: 'palmier', 1: 'maison'}`.\n",
    "\n",
    "**Important** : ajustez la valeur `resolution` (en cm/pixel) selon la r√©solution r√©elle de vos orthophotos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe0a5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import json\n",
    "\n",
    "# Charger le mod√®le ONNX\n",
    "onnx_model = onnx.load(onnx_path)\n",
    "\n",
    "# --- Supprimer toutes les m√©tadonn√©es existantes (ajout√©es par Ultralytics) ---\n",
    "# Deepness fait json.loads() sur CHAQUE valeur ‚Üí il faut que tout soit du JSON valide.\n",
    "# Les m√©tadonn√©es Ultralytics ne sont pas au bon format ‚Üí on les supprime.\n",
    "while len(onnx_model.metadata_props) > 0:\n",
    "    onnx_model.metadata_props.pop()\n",
    "\n",
    "# --- M√©tadonn√©es Deepness --- v2 : 2 classes ---\n",
    "# IMPORTANT : toutes les valeurs doivent √™tre encod√©es avec json.dumps()\n",
    "# car Deepness appelle json.loads() sur chaque valeur lue.\n",
    "# R√©f: https://github.com/PUTvision/qgis-plugin-deepness/blob/devel/tutorials/detection/cars_yolov7/car_detection__prepare_and_train.ipynb\n",
    "\n",
    "class_names = {0: 'palmier', 1: 'maison'}\n",
    "\n",
    "m1 = onnx_model.metadata_props.add()\n",
    "m1.key = 'model_type'\n",
    "m1.value = json.dumps('Detector')\n",
    "\n",
    "m2 = onnx_model.metadata_props.add()\n",
    "m2.key = 'class_names'\n",
    "m2.value = json.dumps(class_names)         # ‚Üí '{\"0\": \"palmier\", \"1\": \"maison\"}'\n",
    "\n",
    "m3 = onnx_model.metadata_props.add()\n",
    "m3.key = 'resolution'\n",
    "m3.value = json.dumps(30)                  # cm/pixel ‚Äî √† adapter !\n",
    "\n",
    "m4 = onnx_model.metadata_props.add()\n",
    "m4.key = 'det_conf'\n",
    "m4.value = json.dumps(0.3)\n",
    "\n",
    "m5 = onnx_model.metadata_props.add()\n",
    "m5.key = 'det_iou_thresh'\n",
    "m5.value = json.dumps(0.5)\n",
    "\n",
    "m6 = onnx_model.metadata_props.add()\n",
    "m6.key = 'det_type'\n",
    "m6.value = json.dumps('YOLO_Ultralytics')\n",
    "\n",
    "# Sauvegarder le mod√®le ONNX avec les m√©tadonn√©es\n",
    "output_onnx_path = os.path.join(DATASET_DIR, 'palmier_maison_yolov8n_deepness.onnx')\n",
    "onnx.save(onnx_model, output_onnx_path)\n",
    "\n",
    "print(f\"Mod√®le ONNX v2 sauvegard√© : {output_onnx_path}\")\n",
    "print(f\"Taille : {os.path.getsize(output_onnx_path) / 1024 / 1024:.1f} Mo\")\n",
    "print()\n",
    "print(\"M√©tadonn√©es Deepness ajout√©es :\")\n",
    "for prop in onnx_model.metadata_props:\n",
    "    print(f\"  {prop.key}: {prop.value}\")\n",
    "    # V√©rification : chaque valeur doit √™tre parsable par json.loads\n",
    "    json.loads(prop.value)\n",
    "print(\"\\n‚úÖ Toutes les valeurs sont du JSON valide.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f895ea",
   "metadata": {},
   "source": [
    "## 12. V√©rifier les m√©tadonn√©es du mod√®le ONNX\n",
    "\n",
    "On relit le fichier ONNX pour confirmer que les m√©tadonn√©es sont bien enregistr√©es (2 classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b3dbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rification\n",
    "check_model = onnx.load(output_onnx_path)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"M√©tadonn√©es du mod√®le ONNX v2 export√©\")\n",
    "print(\"=\" * 50)\n",
    "for prop in check_model.metadata_props:\n",
    "    print(f\"  {prop.key}: {prop.value}\")\n",
    "print()\n",
    "\n",
    "# V√©rifier que les 2 classes sont bien pr√©sentes\n",
    "loaded_classes = json.loads(\n",
    "    [p for p in check_model.metadata_props if p.key == 'class_names'][0].value\n",
    ")\n",
    "assert len(loaded_classes) == 2, f\"‚ùå Attendu 2 classes, trouv√© {len(loaded_classes)}\"\n",
    "print(f\"‚úÖ Classes dans le mod√®le : {loaded_classes}\")\n",
    "\n",
    "print(f\"\\nEntr√©es du mod√®le :\")\n",
    "for inp in check_model.graph.input:\n",
    "    shape = [d.dim_value if d.dim_value else d.dim_param for d in inp.type.tensor_type.shape.dim]\n",
    "    print(f\"  {inp.name}: {shape}\")\n",
    "print(f\"Sorties du mod√®le :\")\n",
    "for out in check_model.graph.output:\n",
    "    shape = [d.dim_value if d.dim_value else d.dim_param for d in out.type.tensor_type.shape.dim]\n",
    "    print(f\"  {out.name}: {shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db6e579",
   "metadata": {},
   "source": [
    "## 13. T√©l√©charger le mod√®le (optionnel)\n",
    "\n",
    "Le mod√®le est d√©j√† sauvegard√© dans Google Drive √† l'emplacement :\n",
    "\n",
    "```\n",
    "Google Drive/Recherche/dataset_palmiers/palmier_maison_yolov8n_deepness.onnx\n",
    "```\n",
    "\n",
    "Vous pouvez aussi le t√©l√©charger directement depuis Colab :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816a2025",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# D√©commenter la ligne suivante pour t√©l√©charger le mod√®le\n",
    "# files.download(output_onnx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eb5c79",
   "metadata": {},
   "source": [
    "## Utilisation dans QGIS avec Deepness\n",
    "\n",
    "1. Ouvrez QGIS et installez le plugin **Deepness** depuis le gestionnaire d'extensions\n",
    "2. Chargez votre orthophoto dans QGIS\n",
    "3. Lancez Deepness : **Plugins > Deepness > Detection**\n",
    "4. S√©lectionnez le fichier `palmier_maison_yolov8n_deepness.onnx`\n",
    "5. Les param√®tres (confiance, IoU, r√©solution) seront automatiquement lus depuis les m√©tadonn√©es\n",
    "6. Lancez l'inf√©rence ‚Äî les **palmiers** et **maisons** d√©tect√©s appara√Ætront comme une couche vectorielle avec leur classe respective\n",
    "\n",
    "**Changement v1 ‚Üí v2** : le mod√®le distingue maintenant les palmiers (classe 0) des maisons (classe 1), ce qui permet de filtrer et visualiser chaque type d'objet s√©par√©ment dans QGIS."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.14.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
